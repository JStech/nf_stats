\documentclass{paper}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\title{Old wine in new bottles: Netfiow and the sampling of species problem}
\author{S. Zabell, D. Scott, J. Stechschulte}

\begin{document}
\maketitle

\section{Introduction}
The Wikipedia tells us that ``NetFlow is a network protocol developed by Cisco
Systems for collecting IP traffic information.'' In this paper we shall use the
term in a slightly more general sense, referring to data about flows generated
by grouping packets on the basis of metadata.

Netflow is typically generated by network routers. A router's primary job is, of
course, routing network traffic, which leaves generating netflow as a secondary
concern. Since the resources available for generating netflow may be limited,
sampling is a common strategy for coping with large data volumes. The full flow
of packets is sampled at some rate---typically 1~in~100---and this sampled
stream is used to make inferences about the original flows in the full stream.
The details of this sampling, discussed below, are relevant to interpreting the
resulting netflow records. In this paper we investigate the statistical effects
of sampling, particularly on small flows (those transmitted by only a few
packets).

\section{Data}
Our data is two minutes of anonymized packet traces from the Center for Applied
Internet Data Analysis Anonymized Internet Traces 2015 dataset. Specifically,
the data was collected 19 February, 2015, starting at 1300 UTC\@. Only IPv4
packets using TCP or UDP were considered. This subset comprised 37,607,469
packets, with a total of 2.27~GB transmitted. (IP and TCP/UDP headers were not
included in the packet size.)

From this dataset we generated netflow-like output by matching five-tuples and
summarizing the number of packets, total bytes, and start and end timestamps.
Although it is conceivable that a flow ended (either by timing out or by sending
a TCP FIN or RST) and a new flow began with the same five-tuple, this is
extremely unlikely and would not have a significant impact on the results.  The
original data contained 1,163,780 flows.

We also simulated four different methods of sampling the packet stream, and
generated netflow for each. Our sampling rate was 1/100, a typical choice for
netflow generation. The four methods are referred to below as ``deterministic'',
``independent'', ``buffered'', and ``simple''. The ``deterministic'' sample is
simply every lOOth packet, beginning with the first packet. The ``independent''
sample was generated by sampling each packet independently with a probability of
1/100. The ``buffered'' sample was generated by filling a buffer with 100
consecutive packets, and selecting one at random; it is most similar to what
real routers typically do. Finally, the ``simple'' sample was a simple random
sample: 376,074 packets chosen at random without replacement. No sampling method
produced significantly different results from the others. Except where otherwise
stated, the simple sample was used for the analysis below.

The sampled packet streams were used to generate flows in the same manner as the
unsampled stream. The flows generated from the sampled packets we call ``proxy''
flows, as they are proxies for the true flows that the packets were sampled
from. We do this knowing that ``proxy'' is already used with another meaning in
computer networking. To be explicit, we have nothing to say about web proxies,
or other type of computer proxy.

\begin{table}
\begin{center}
\begin{tabular}{r r r r r}
$r$ & Simple & Buffered & Deterministic & Independent \\
\midrule
1 & 63691 & 63648 & 64261 & 63872 \\
2 & 8859 & 8896 & 8842 & 8802 \\
3 & 3335 & 3435 & 3408 & 3522 \\
4 & 2132 & 2034 & 2117 & 2035 \\
5 & 1561 & 1558 & 1525 & 1519 \\
6 & 1211 & 1208 & 1206 & 1155 \\
7 & 943 & 965 & 962 & 985 \\
8 & 768 & 795 & 788 & 797 \\
9 & 673 & 710 & 641 & 685 \\
10 & 573 & 588 & 563 & 602 \\
11 & 499 & 467 & 484 & 467 \\
12 & 404 & 405 & 417 & 404 \\
13 & 355 & 338 & 383 & 365 \\
14 & 318 & 300 & 333 & 318 \\
15 & 267 & 292 & 290 & 245 \\
16 & 264 & 257 & 238 & 241 \\
17 & 231 & 225 & 210 & 229 \\
18 & 204 & 180 & 199 & 192 \\
19 & 168 & 178 & 167 & 156 \\
20 & 164 & 161 & 170 & 148 \\
\end{tabular}
\caption{Number of flows with~$r$ packets sampled}
\label{flows_with_r}
\end{center}
\end{table}

Table~\ref{flows_with_r} summarizes one realization of the data by showing how
many flows had~$r$ packets sampled, for each sampling method and~$r = 1 \dots
20$. This will prove useful later. The counts decrease in a way reminiscent of
``Zipf's law'' (that is, a power law), although the log counts only decrease in
truly linear fashion after the first 5 or so.

\section{The problem}

A proxy flow can be used to estimate the number of packets and bytes in the
original flow giving rise to it. The naive method for generating such
estimations is simply multiplying the sampled number of packets and bytes by 100
(or more generally the reciprocal of the sampling rate, whatever that might be).

\begin{table}
\begin{center}
\begin{tabular}{r r r r r r}
$r$ & Actual mean & Naive estimate & Error & New estimate & Error \\
\midrule
 1 &   29 &  100 & 245\% &   28 & -3\% \\
 2 &  117 &  200 &  71\% &  113 & -3\% \\
 3 &  246 &  300 &  22\% &  256 &  4\% \\
 4 &  361 &  400 &  11\% &  366 &  1\% \\
 5 &  462 &  500 &   8\% &  465 &  1\% \\
 6 &  560 &  600 &   7\% &  545 & -3\% \\
 7 &  654 &  700 &   7\% &  652 & -0\% \\
 8 &  758 &  800 &   6\% &  789 &  4\% \\
 9 &  837 &  900 &   8\% &  851 &  2\% \\
10 &  927 & 1000 &   8\% &  958 &  3\% \\
11 & 1042 & 1100 &   6\% &  972 & -7\% \\
12 & 1139 & 1200 &   5\% & 1142 &  0\% \\
13 & 1224 & 1300 &   6\% & 1254 &  2\% \\
14 & 1312 & 1400 &   7\% & 1259 & -4\% \\
15 & 1458 & 1500 &   3\% & 1582 &  9\% \\
16 & 1497 & 1600 &   7\% & 1488 & -1\% \\
17 & 1604 & 1700 &   6\% & 1590 & -1\% \\
18 & 1663 & 1800 &   8\% & 1565 & -6\% \\
19 & 1841 & 1900 &   3\% & 1952 &  6\% \\
\end{tabular}
\caption{Mean flow sizes in number of packets}
\label{mean_flow_size}
\end{center}
\end{table}

How well does this rule work? Using the true netflow generated from the
unsampled data, we generated Table~\ref{mean_flow_size} comparing the estimated
and actual number of packets, as well as a new estimate to be explained shortly.

It is apparent the ``multiply by 100'' rule performs very poorly for small
flows, overestimating the true mean by a factor of more than two in the case of
proxy flows containing only a single packet. The relative magnitude of the error
does decrease as the observed number of packets increases, but is still
considerable even in the case of 5 packet proxy flows, overestimating the true
mean by 8\%. (It should be remembered the sample sizes in question are very
large, so classical sampling variation per se should be small.)

Why is the intuitively natural approach of multiplying by the reciprocal of the
sampling rate so badly off? There are two complications here.

\subsection{The ``claims and policies'' scenario}

The following type of situation is often encountered in classical sampling.  One
is interested in, say, auditing insurance policies. One or more claims can be
submitted, covered under a single policy. The claims (as opposed to policies)
are readily accessed, being enumerated in a computer file. In our problem, the
``claims'' are the packets, the ``policies'' the flows, and we sample
claims/packets because the packets constitute the sampling frame, that is, what
is accessible to us.

Note the probability of a sampled packet coming from a flow is proportional to
the size of that flow (i.e., the number of packets in it). Thus, for example, if
one flow consists of two packets and another ten, then it is five times more
likely that a packet will be selected from the second flow than the first. Thus
there is an inherent size-bias in the sampling: longer flows are more likely to
be seen than shorter flows.

By itself this need not be a problem. In the claims and policies scenario, if a
claim is sampled, then typically both the governing policy and any other claims
filed under that policy are easily determined. Thus, once we see a claim, we
know how many other claims are associated with it, and so we can weight it
accordingly (inversely proportional to the number of other claims associated
with the policy).

Unfortunately this workaround is not available to us here: it would mean here
that given one or more packets in the flow, we would know (or easily learn) the
flow in its entirety, which is not true. (That is pecisely why the number of
packets and bytes in the true flow has to be estimated.) This means that any
extrapolation to flows cannot be directly done solely from information in
netflow. We have a random sample of packets, not flows.

\subsection{The ``sampling of species'' paradigm}

Instead, we are in the sampling of species situation, as it is often called.
Imagine you land in a new and unexplored area, observe different animals, and
classify them into the different species they belong to. At any given time you
may have seen~$j_1$ of one species~$s_1$,~$j_2$ of another~$s_2$, and so on.
What you would really like to know is the total number of members of
species~$s_k$ there actually are, both observed and unobserved, for each~$k$.

This is \emph{exactly} our problem, not just one similar or analogous to it.
The isomorphism is:
$$ packets \longleftrightarrow animals \; flows \longleftrightarrow species~$$

\section{The sampling of species problem}

There is an enormous literature on various aspects of the sampling of species
problem (SOSP). This paper will discuss some aspects of this literature, and how
it can or might be applied to flow estimation.

\subsection{Brief historical perspective}

The public literature on the SOSP is often traced back to a famous paper of R.
A. Fisher: Corbet, Fisher, and Williams (1943). In it Fisher proposed a simple
parametric model that often works quite well. If~$x_1, x_2, \dots$ are the
counts for a succession of species ($x_1$ for the first species, and so on),
then Fisher's model is that the \emph{species} counts follow a \emph{compound
Poisson distribution}. Informally, imagine that for each species~$s$, there is a
random parameter~$\lambda_s$, chosen from a distribution~$G$, and that~$X_s$
instances of the species are then observed,~$X_s$ having a Poisson distribution
with parameter~$\lambda_s$.

In the very special case that the species counts are generated by a single
Poisson variate having parameter~$\lambda$, this means that
$$ P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}. $$
In general, if~$n_x$ denotes the number of species for which there are~$x$
instances in a sample of size~$n$ ($x \geq 1$), and there are~$S$ species in
total (so that~$n = n_1 + \cdots + n_s$), then it is not hard to see that
$$ \eta_x := E[n_x] = S \int_0^\infty \frac{\lambda^x e^{-\lambda}}{x!} dG(\lambda), $$
where~$G$ is some suitable distribution. In particular, Fisher assumed that~$G$
is a gamma distribution, having a density function of the form
$$ g_{\alpha\beta}(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1}e^{-\lambda/\beta}. $$

In that case, one has, setting~$\gamma = \beta/(1+\beta)$,
$$ \eta_x = \eta_1 \frac{\Gamma(x + \alpha)}{x!\Gamma(1+\alpha)}\gamma^{x-1},~x \geq 1 $$
which is proportional to the negative binomial distribution with
parameters~$\alpha$ and~$\gamma$ (but limited to positive values of~$x$).

Since then, such methods have been applied to a variety of problems, such as
words counts, with word ``tokens'' and ``types'' taking the place of animals and
species. One well-known example: three decades after Fisher's paper, Efron and
Thisted (1976) wrote a paper, ``Estimating the number of unseen species: how
many words did Shakespeare know?'' (Somewhat surprisingly, Fisher's model does
quite well in this setting.)

Although of great statistical interest, much of this literature is of limited
applicability to our problem, and that is for two reasons. First, much of it is
devoted to estimating ``species richness''; in our setting, the total number of
flows in our take. This is a problem ecologists, for example, are interested in
(measures of species diversity), but not us. (An additional complication here is
that when the sample is only 1/100 or less of the total population, accurately
estimating the number of distinct species is not really possible: estimators
exist, but the attached standard deviations are prohibitively large.) Second,
some of these methods (not all) involve parametric models that may not apply to
our type of data. In general, given the diverse nature of our data sources,
nonparametric approaches are better.

\subsection{The Turing-Good estimator}

todo: introduce Good (1953) paper

\subsubsection{Defining the estimator}\label{def_tg}

Good says (p. 238) ``we shall be concerned with~$q_r$, the population frequency
of an arbitrary species that is represented~$r$ times in the sample.'' This is,
then, precisely the question of determining the size of a flow (i.e., the number
of packets in it), given we have observed~$r$ packets in our sample. As Good
notes earlier (p. 237), ``If a particular species is represented~$r$ times in
the sample of size~$n$, then~$r/n$ is not a good estimate of the population
frequency,~$p$, when~$r$ is small.'' We have already encountered this
phenomenon. To see that this is just the~$100r$ estimator in disguise, note that
if~$N$ is the total number of packets from which our sample is drawn, then the
estimated fraction of the population consisting of the flows corresponding
to~$100r$ is $$ \frac{100r}{N} = \frac{N}{n}\frac{r}{N} = \frac{r}{n}. $$

The alternative estimator Good discusses ``was first suggested to me, together
with an intuitive demonstration, by Dr.\ A.\ M.\ Turing several years ago''. It
replaces~$r$ in~$r/n$ by
$$ r* = (r + 1) \frac{n_{r+1}}{n_r}. $$
This was the estimator used to generate the last column of
Table~\ref{mean_flow_size} (using~$100r^*$ in place of~$100r$), and it not only
performs better, but indeed provides a very good approximation to the actual
value. (Strictly speaking, we are estimating the expected value of the
population frequency, and using that as the best point estimate for an
individual frequency.)

\emph{Note}: In Good's paper he uses~$N$ instead of our~$n$. Since the former is
usually used to denote the population size, we have reverted to the more common
notation.

\subsubsection{Heuristic motivation}

The sample proportion~$r/n$ is an unbiased estimator for the corresponding
population proportion, so it may seem surprising that an adjustment such as
Turing's is needed. A number of derivations for Turing's estimator appear in the
literature; see Good (1993) for references to several. Here we limit ourselves
to a few remarks of a heuristic nature, suggesting why the adjustment is needed.

First, to say~$r/n$ is unbiased is to say that its average over repeated samples
gives the correct value. Included in such samples would be the zero sample, the
sample containing no packets in the flow, or animals of the given species. But
of course in this case we would not compute the sample proportion in the first
case, since we do not even know the flow or species even exists! The relevant
average is really one over samples containing at least one packet in the flow or
animal in the species. Thus the sample proportion, being always positive, will
exhibit a positive upwards bias, and some adjustment downwards is needed.  This
is precisely the function of Turing's factor.

Second, suppose we actually did know beforehand all possible species. Then, as
Good notes in his 1953 paper, the unadjusted sample proportions (summed over all
observed species) will assign a zero probability to those species that did not
appear in our sample, clearly an unsatisfactory state of affairs. But as an
immediate corollary of this, if the sample proportion underestimates the
frequency of the unobserved species, it must necessarily overestimate the
frequency of the observed species. Once again, it is apparent some adjustment
downwards is needed.

\subsection{Effect of sampling scheme}

In general, there appears to be little difference in results among the different
sampling schemes. Absent substantial bias, sample sizes appear to swamp any
limited inefficiencies in say, deterministic versus random sampling.

\begin{table}
\begin{center}
\begin{tabular}{r r r r r r}
$r$ & Simple & Buffered & Deterministic & Independent & True \\
\midrule
 1 &  28 &  28 &  28 &  28 &  29 \\
 2 & 113 & 116 & 116 & 120 & 117 \\
 3 & 256 & 237 & 248 & 231 & 246 \\
 4 & 366 & 383 & 360 & 373 & 361 \\
 5 & 465 & 465 & 474 & 456 & 462 \\
 6 & 545 & 559 & 558 & 597 & 560 \\
 7 & 652 & 659 & 655 & 647 & 654 \\
 8 & 789 & 804 & 732 & 774 & 758 \\
 9 & 851 & 828 & 878 & 879 & 837 \\
10 & 958 & 874 & 946 & 853 & 927 \\
\end{tabular}
\caption{Turing-Good estimates for mean flow sizes in number of packets}
\label{tg_est}
\end{center}
\end{table}

Table~\ref{tg_est} illustrates this, giving the estimates for the mean number of
packets in flows corresponding to~$r$-packet proxy flows, for~$r = 1, \dots,
10$.

\subsection{A reality check}

Another question that Good's 1953 paper also addresses is: what is the chance
that the next sampled animal is a member of a previously seen species versus an
unobserved species? Or put another way, what fraction of packets in the original
stream lie in sampled versus unsampled flows? Recall that
$$ \sum_{r \geq 1} r \cdot n_r = n. $$
The Turing-Good estimator suggests a plausible estimate of the overall fraction
of packets lying in sampled flows (that is, the fraction of the total take of
packets that lie in actual flows---not proxy flows---one or more of whose
packets were sampled) is
\begin{align*}
  \frac{n_1 \cdot 1^* + n_2 \cdot 2^* + n_3 \cdot 3^* + \dots}{n}
  &= \sum_{r \geq 1} n_r \frac{(r + 1)}{n} \frac{n_{r+1}}{n_r} \\
  &= \frac{\sum_{r \geq 2} r \cdot n_r}{n} \\
  &= \frac{n - n_1}{n} \\
  &= 1 - \frac{n_1}{n}.
\end{align*}

\begin{table}
\begin{center}
\begin{tabular}{l r r}
Sampling method & Estimate & Actual   \\
\midrule
Simple          & 0.169358 & 0.168284 \\
Buffered        & 0.169243 & 0.167398 \\
Deterministic   & 0.170873 & 0.167483 \\
Independent     & 0.170873 & 0.168306 \\
\end{tabular}
\caption{Fraction of packets in unsampled flows}
\label{uns_flows}
\end{center}
\end{table}

Recall that~$r^*$ was defined in Section~\ref{def_tg}. Thus~$n_1/n$ estimates
the fraction of packets in the total take that lie in the unsampled flows. As a
check, we decided to see how well this did on our data. The results, shown in
Table~\ref{uns_flows}, were gratifying.

It is apparent both that the Turing-Good methodology is precisely what is
required in this situation and that the method of sampling is unimportant (no
surprise given the sample size).

\emph{Note}: Good discusses at some length smoothing the~$n_r$, replacing them
by smoothed values~$n'_r$ in the above formulas. This technique is less
important here, given the sample sizes we are dealing with (but should be kept
in mind when dealing with smaller samples, as illustrated in some of the
examples in Good's paper).

\subsection{Estimating other population parameters}

In his 1953 paper Good also discusses how to estimate other population
parameters such as higher moments and measures of diversity. The expressions he
derives do \emph{not} require replacing~$r$ by~$r^*$ in order to obtain unbiased
estimates.  (Or any other adjustment, including the ``multiply by 100'' rule.
One of Good's purposes in this section is presumably to demonstrate instead the
efficacy of his smoothing techniques.)

Among the parameters discussed by Good are measures of population heterogeneity.
These may be of interest in our problem as a means of providing situational
awareness. Although the expression for estimating the Shannon entropy is complex
and may not be suited·to the streaming context, another measure, the classical
repeat rate, is easily evaluated.

If~$p_1, \dots, p_t$ is a sequence of population frequencies (so that~$\sum_j
p_j = 1$), then the \emph{repeat rate} for the population is
$$ \rho = \sum_{j=1}^t p_j^2. $$
Just as in classical sampling, an unbiased estimator for~$\rho$ is
$$ \hat{\rho} = \frac{1}{n(n-1)} \sum_r r(r-1)n_r $$
(Good, 1953, p. 245, Equation 30).

How well does this estimator perform on our data? The agreement is excellent:
$$ \rho = 0.000187908,\; \hat{\rho}= 0.000188785, $$
a relative absolute error of less than 0.5\% (estimate based on the simple
random sample).

\emph{Remark 1}: The fact that~$\hat{\rho}$ is an unbiased estimator of~$\rho$
is \emph{not} a consequence of the classical theory based on simple random
sampling (SRS). As we have repeatedly stressed, in the SOS, matters are often
very different from SRS, and each estimation problem has to be examined afresh.
It is in fact somewhat remarkable that the biased proxy sample counts~$r$ can be
used directly in the estimation of the repeat rate, Shannon entropy, moments,
etc.

\emph{Remark 2}: The value of~$\rho$ reflects the highly heterogeneous nature of
the population of flows: a large number of single-packet flows, and a small
number of very large flows. One might explore filtering out most or all of the
single-packet flows in calculating~$\rho$, and see how this affects the
sensitivity of this diagnostic for changes in the underlying traffic. (For some
discussion on how this might be done, see section 6.)

\section{Estimating bytes: an unresolved problem}

\begin{table}
\begin{center}
\begin{tabular}{r r r r}
$r$ & Estimate (kB) & True (mean) & Ratio \\
\midrule
1 &  27 &  13 & 2.07692 \\
2 &  93 &  58 & 1.60345 \\
3 & 141 & 111 & 1.27027 \\
4 & 185 & 169 & 1.09467 \\
5 & 241 & 233 & 1.03433 \\
\end{tabular}
\caption{Mean flow sizes in bytes}
\label{flow_sizes}
\end{center}
\end{table}

In addition to estimated packet counts, our netflow data also contains estimates
of the total number of bytes in a flow. These are likewise naively generated by
multiplying the reciprocal sampling rate by the total bytes in packets in the
proxy flow. Estimating the number of bytes in this way is also seriously flawed,
as shown in Table~\ref{flow_sizes}.

\emph{Conclusion}: There are apparently serious inaccuracies in our current
estimate of the number of bytes in a flow when~$r$ is small.

This problem cannot be fixed by the simple expedient of downweighting the
currently used byte estimate by the same Turing factors used earlier to estimate
the number of packets. For example, for~$r = 1$, the empirically determined
downweighting factor for our data was found to be 28\%; applying this to the
estimate of 27~kB gives a revised estimate of 7.6~kB. This underestimates the
true mean by 42\%.

\subsection{Ratio estimation}

Understanding what is going on here involves a few subtleties. Suppose~$U$
and~$V$ are two positive random quantities with means~$\mu = E[U], \nu = E[V]$.
Then in general~$E[U/V]$ differs from~$E[U]/E[V]$; more prosaically, an average
of ratios usually differs from the corresponding ratio of averages.

This is easily illustrated using our data. Restricting ourselves to TCP flows
with only one packet sampled (with non-zero size), one can look at the number
of bytes in the proxy flow and the number of bytes in the actual flow. Dividing
the latter by the former (actual bytes divided by proxy bytes), the average of
the resulting ratios over all proxy flows is 99---that is, on average a flow
contained 99 times more bytes than its corresponding proxy flow. Thus, if one
wishes to estimate the true number of bytes \emph{in an individual flow} from
the observed number of bytes in the corresponding proxy generated by the
sample, then multiplying the latter by the reciprocal sampling fraction of 100
performs fairly well ``on average''. (This is not really surprising, since we
are in something analogous to the ``large r'' setting for packets. Note however
that although the average of the ratios is close to the reciprocal sampling
fraction of 100, the variability of the ratios about 100 is quite substantial.)

In contrast, suppose we look at the average number of bytes in the proxies and
the average number of bytes in the true flows; dividing the second by the first
results in a ratio of 37, substantially less than 37: the ratio of averages is
indeed different from the average of the ratios.

Thus if we wish to estimate the second average using the first (that is, the
average number of bytes in the actual flows giving rise to the one-packet
proxies, using the average number of bytes in the one-packet proxies) some form
of adjustment analogous to the Turing-Good estimator is necessary. One
adjustment that performed well on our TCP-flow data was, for proxy flows
with~$r$ packets, to multiply the average number of bytes in the~$r$-packet
proxies by 100 \emph{times} the sum of the bytes in the~$(r + 1)$-packet proxies
($s_{r+i}$) divided by the sum of the bytes in the~$r$-packet proxies ($s_r$)·

\begin{table}
\begin{center}
\begin{tabular}{r@{\hskip 1.7em}r r@{\hskip 1.7em}r r@{\hskip 1.7em}r r}
    & \multicolumn{2}{c}{True flow size} & \multicolumn{2}{c}{Proxy flow size} & Ratio of & \\
$r$ & sum & mean & sum ($s_r$) & mean & means &~$100 \times s_{r+1}/s_r$ \\
\midrule
1 & 785,960,654 & 8734 & 16,573,856 & 184 & 47 & 48 \\
2 & 481,432,787 & 5350 &  7,885,666 &  88 & 61 & 57 \\
3 & 346,406,043 & 3850 &  4,458,256 &  50 & 78 & 83 \\
4 & 333,496,886 & 3706 &  3,696,528 &  41 & 90 & 93 \\
5 & 328,578,376 & 3651 &  3,428,908 &  38 & 96 & 89 \\
\end{tabular}
\caption{Estimator for flow size, in bytes, of TCP flows}
\label{size_est}
\end{center}
\end{table}

Table~\ref{size_est} gives the result (TCP-flows only) for~$r = 1, \dots, 5$:

\subsection{Rationale}

We do not yet have a principled justification for this last adjustment (similar
to the ones that can be advanced for the Turing-Good estimator) but the
following hand-waving argument sounds plausible.

Let us rewrite the classical Turing-Good expression for~$r^*$ in the following
natural way:
$$ r^* =(r + 1) \frac{n_{r+1}}{n_r} = r \frac{(r+1)n_{r+1}}{rn_r}. $$
That is, the species count~$r$ is adjusted by a factor whose denominator is the
total number of packets in the~$r$-th class (consisting of all~$r$-packet proxy
flows) and whose numerator is the total number of packets in the next (i.e., the
(r+ 1)- st) class.

In the case of bytes we are doing the same thing, with a ratio whose numerator
and denominator are the total number of bytes in two ``adjacent classes''.
Indeed, if we were sampling bytes instead of packets, we now see this is exactly
what the Turing-Good estimator would tell us to do. The difference here is that
we are not sampling bytes using simple random sampling, but in clusters, but
that does not seem to make much difference. (Except that, just as in classical
cluster sampling, the variance of the estimator can be expected to increase.
Cluster sampling in general involves a trade-off between convenience and
precision.) Of course our ``classes'' are also non-standard, but this argument
is intended only as a suggestive heuristic, not an actual derivation.

\subsection{Bytes vs.\ packets}

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{fig1.png}
\caption{Scatterplot of bytes versus packets for flows with one packet sampled}
\label{pb_plot}
\end{center}
\end{figure}

The plot in Figure~\ref{pb_plot} contains two distinctive features:
\begin{enumerate}
  \item a maximum number of bytes corresponding to a given number of packets,
  and a minimum of 0 bytes
  \item a ``feast-or-famine'' effect often present that is evident on the
  right.
\end{enumerate}
Figure~\ref{pkt_size_dist} shows the distributions of average packet sizes for flows which
had one packet sampled in the two cases where the original flow had 5 or 25
packets. Even accounting for sampling fluctations, it is apparent the two
distributions are different: while the 25-packet flows show some feast-or-famine
effect, the 5-packet flows appear to be all famine.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.35]{fig2.png}
\caption{Distributions of average packet size for 5 and 25 packet flows that
had a single packet sampled.}
\label{pkt_size_dist}
\end{center}
\end{figure}

Of course \emph{how} different such distributions are will likely vary
substantially from one population of flows to another; hard-and-fast rules may
well prove elusive. Nevertheless, it may be possible to leverage additional
information from netflow data, such as considering protocol and packet flags, to
improve estimates of the number of bytes in a packet.

\section{DNS:\ Public enemy number one}

It is a standard observation in the SOS literature that the presence of large
numbers of rare species complicates any attempt at statistical estimation or
extrapolation. This has obvious implications for the use of netflow data, since
many flows consist of a single packet.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.7]{fig3.png}
\caption{Proportion of flows that are on port 53 and that have only a single
packet.}
\label{dns_vs_single}
\end{center}
\end{figure}

One source of this problem is DNS, which rapidly became public enemy number one.
Why? Two sets of closely related statistics, illustrated in
Figure~\ref{dns_vs_single}, make clear the magnitude of the problem.

\begin{enumerate}
  \item Out of a total of 1,163,780 flows, 413,393 contained only one packet;
  that is, fully 36\%. Of these, 130,639 (32\%) had port 53 as either source or
  destination port.
  \item Of 1,163,780 flows, 148,690 flows had either source or destination port
  53: that is 12\%. Of these, 88\% (130,639) consisted of a single packet.
\end{enumerate}
So, over a third of the flows consist of a single packet, many of these ($\sim
32\%$) involve port 53; of the flows involve port 53 nearly all (88\%) consist
of a single packet.

The moral seems clear: one should treat DNS separately. It consists of largely
predictable, highly idiosyncratic traffic that has the effect of dumping a huge
amount of noise on whatever signal may be present in the remaining data. (This
is not to say one should ignore DNS, since it plays an essential role in many
network activities; just that it is so different from almost all other packets
and flows as to warrant separate treatment.)

\emph{Caution}: These results need to be replicated on other data sets but seem
likely to hold true in general, at least on a qualitative level. Todo: I think
we've done this now.

\section{References}
Efron, B., Thisted, R. ``Estimating the number of unseen species: How many words
did Shakespeare know?'' 1976.

Fisher, R.A. ``Part 3. A theoretical distribution for the apparent abundance of
different species.''

Good, l.J. ``The population frequencies of species and the estimation of
population parameters.'' 1953.

Good, l.J., Toulmin, G.H. ``The Number of New Species, and the Increase in
Population Coverage, when a Sample is Increased.'' 1956.

Good, l.J. ``Turing's anticipation of empirical Bayes in connection with the
cryptanalysis of the Naval Enigma.'' 1999.

Thisted, R., Efron, B. ``Did Shakespeare Write a Newly-Discovered Poem?'' 1987.
\end{document}
